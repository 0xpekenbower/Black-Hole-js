# input {
#   file {
#     path => "/logs/setup.log"
#     start_position => "beginning"
#     sincedb_path => "/dev/null"
#     tags => ["setup"]
#   }
#   file {
#     path => "/logs/nginx.log"
#     start_position => "beginning"
#     sincedb_path => "/dev/null"
#     tags => ["nginx"]
#   }
#   file {
#     path => "/logs/redis.log"
#     start_position => "beginning"
#     sincedb_path => "/dev/null"
#     tags => ["redis"]
#   }
#   file {
#     path => "/logs/postgres_db.log"
#     start_position => "beginning"
#     sincedb_path => "/dev/null"
#     tags => ["postgres_db"]
#   }
#   file {
#     path => "/logs/pgadmin.log"
#     start_position => "beginning"
#     sincedb_path => "/dev/null"
#     tags => ["pgadmin"]
#   }
#   file {
#     path => "/logs/kafka.log"
#     start_position => "beginning"
#     sincedb_path => "/dev/null"
#     tags => ["kafka"]
#   }
#   file {
#     path => "/logs/gateway.log"
#     start_position => "beginning"
#     sincedb_path => "/dev/null"
#     tags => ["gateway"]
#   }
#   file {
#     path => "/logs/auth.log"
#     start_position => "beginning"
#     sincedb_path => "/dev/null"
#     tags => ["auth"]
#   }
#   file {
#     path => "/logs/chat.log"
#     start_position => "beginning"
#     sincedb_path => "/dev/null"
#     tags => ["chat"]
#   }
#   file {
#     path => "/logs/dash.log"
#     start_position => "beginning"
#     sincedb_path => "/dev/null"
#     tags => ["dash"]
#   }
#   file {
#     path => "/logs/game.log"
#     start_position => "beginning"
#     sincedb_path => "/dev/null"
#     tags => ["game"]
#   }
#   file {
#     path => "/logs/frontend.log"
#     start_position => "beginning"
#     sincedb_path => "/dev/null"
#     tags => ["frontend"]
#   }
# }

# filter {
#   # The logs are lines like: filename.log: [timestamp, {json}]
#   # So extract JSON part after the timestamp array, e.g.:
#   # nginx.log: [1751258102.000000000, {...}]
#   grok {
#     match => { "message" => "^\S+\.log: \[\d+\.\d+, %{GREEDYDATA:json_payload}\]$" }
#     remove_field => ["message"]
#   }

#   # Parse the JSON payload extracted by grok
#   json {
#     source => "json_payload"
#     target => "payload"
#     remove_field => ["json_payload"]
#   }

#   # Optional: If the payload contains a nested JSON string in 'log' field, parse it
#   if [payload][log] and [payload][log] =~ /^\{.*\}$/ {
#     json {
#       source => "[payload][log]"
#       target => "inner_log"
#     }
#   }

#   # Use timestamp from inner_log.time_local if available, otherwise use @timestamp as is
#   if [inner_log][time_local] {
#     date {
#       match => ["[inner_log][time_local]", "dd/MMM/yyyy:HH:mm:ss Z"]
#       target => "@timestamp"
#     }
#   }

#   # Move useful fields to root level for easier querying
#   mutate {
#     rename => { "payload" => "event_data" }
#   }
# }

# output {
#   elasticsearch {
#     hosts => ["http://elasticsearch:9200"]
#     index => "%{[tags][0]}-%{+YYYY.MM.dd}"
#     user => "elastic"
#     password => "${ELASTIC_PASSWORD}"
#     ilm_enabled => false
#   }

#   # stdout {
#   #   codec => rubydebug
#   # }
# }
input {
  tcp {
    port => 5000
    codec => json_lines
  }
}

# Minimal filter: only route by `service` field, no heavy parsing or date manipulation
filter {
  if ![service] {
    mutate {
      add_field => { "service" => "unknown" }
    }
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    user => "elastic"
    password => "${ELASTIC_PASSWORD}"
    index => "%{service}-%{+YYYY.MM.dd}"
    ilm_enabled => false
  }
}
